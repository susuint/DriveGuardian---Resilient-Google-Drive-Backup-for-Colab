# -*- coding: utf-8 -*-
"""google_drive_backup_optimized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/xxxxxyyyyyzzzzzz

# Google Drive Folder Backup Tool - OPTIMIZED VERSION
## Tool for backing up folders from Google Drive with validation, logging and multi-threaded downloads

### Features:
- ‚úÖ Backup entire shared folders from Google Drive
- ‚úÖ Create backup folder with "_BACKUP" suffix
- ‚úÖ File validation before deletion (check size and checksum)
- ‚úÖ JSON logging to avoid duplicate backups
- ‚úÖ Auto cleanup local files after successful upload
- ‚úÖ Retry mechanism for network errors
- ‚úÖ Detailed progress tracking
- üöÄ **NEW: Multi-threaded downloads (3-5 files simultaneously)**
- üöÄ **NEW: Auto-optimize workers based on RAM/CPU**
- üöÄ **NEW: No timeout warnings**
"""

# ============================================================
# ‚öôÔ∏è  MAIN CONFIGURATION - EDIT HERE
# ============================================================

# üìÅ ID of folder to backup (get from Google Drive URL)
# Example: https://drive.google.com/drive/folders/1ZY4ab0XlPHa5_t10XnSvPbWUvJRdN4Nx
SOURCE_FOLDER_ID = 'xxxxxxzzzzzzzzzzyyyyyyyyySOURCEfolderID'

# üìÅ ID of parent folder where backup will be saved (optional)
# Set to None if you want to save in root "My Drive"
BACKUP_PARENT_ID = 'xxxxxxzzzzzzzzzzyyyyyyyyyBACKUPfolderID'

# üè∑Ô∏è  Suffix for backup folder name
FOLDER_SUFFIX = '_BACKUP'

# üöÄ Number of concurrent download threads (None = auto-optimize)
# Recommended values: 3-8 (or leave None for automatic)
MAX_WORKERS = None  # None = auto-detect, or set 4, 6, 8...

# ============================================================

# Install required libraries
!pip install -q google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client tqdm requests psutil

# Import libraries
import os
import json
import hashlib
import time
from datetime import datetime
from pathlib import Path
from tqdm.notebook import tqdm
from google.colab import auth
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
from googleapiclient.errors import HttpError
from google.auth import default
from google.auth.transport.requests import AuthorizedSession
import requests
import io
import concurrent.futures
from threading import Lock
import logging
import psutil
import multiprocessing
import gc  # Garbage collector for memory management

# Disable timeout warnings from google_auth_httplib2
logging.getLogger('google_auth_httplib2').setLevel(logging.ERROR)

# Authenticate with Google Drive
from google.auth import default
from google.auth.transport.requests import Request

auth.authenticate_user()

# Get credentials
creds, _ = default()

# Build service with credentials (no separate HTTP client needed)
drive_service = build('drive', 'v3', credentials=creds)
print("‚úÖ Successfully authenticated with Google Drive!")

class DriveBackupManager:
    """Manages folder backup from Google Drive with multi-threaded downloads"""

    def __init__(self, service, log_file='backup_log.json', max_workers=None):
        self.service = service
        self.log_file = log_file
        self.backup_log = self.load_log()
        self.local_temp_dir = '/content/temp_backup'
        os.makedirs(self.local_temp_dir, exist_ok=True)
        
        # Auto-detect optimal number of workers
        if max_workers is None:
            self.max_workers = self._auto_detect_workers()
        else:
            self.max_workers = max_workers
            
        self.log_lock = Lock()  # Lock for thread-safe logging
        self.download_stats = {'success': 0, 'failed': 0, 'skipped': 0}
        self.upload_stats = {'success': 0, 'failed': 0}
        # Store credentials for creating thread-local services
        self.creds, _ = default()
        
        print(f"üöÄ Number of workers in use: {self.max_workers}")
    
    def __del__(self):
        """Destructor - cleanup resources when object is destroyed"""
        try:
            # Cleanup temp directory
            if hasattr(self, 'local_temp_dir') and os.path.exists(self.local_temp_dir):
                for file in os.listdir(self.local_temp_dir):
                    file_path = os.path.join(self.local_temp_dir, file)
                    try:
                        os.remove(file_path)
                    except:
                        pass
            
            # Force garbage collection
            gc.collect()
        except:
            pass
    
    def _auto_detect_workers(self):
        """Auto-detect optimal number of workers based on RAM and CPU"""
        try:
            # Get RAM information
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024 ** 3)
            
            # Get CPU cores
            cpu_count = multiprocessing.cpu_count()
            
            # Calculate optimal number of workers
            # Each worker needs approximately 200-300MB RAM for buffer
            workers_by_ram = int(available_gb / 0.3)  # 300MB per worker
            
            # Number of workers should not exceed CPU cores
            workers_by_cpu = cpu_count
            
            # Choose minimum but ensure it's within safe range 3-8
            optimal_workers = min(workers_by_ram, workers_by_cpu)
            optimal_workers = max(3, min(optimal_workers, 8))  # Limit to 3-8
            
            print(f"üíæ Available RAM: {available_gb:.1f} GB")
            print(f"üñ•Ô∏è  CPU cores: {cpu_count}")
            print(f"‚öôÔ∏è  Optimal workers: {optimal_workers} (based on RAM: {workers_by_ram}, CPU: {workers_by_cpu})")
            
            return optimal_workers
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Cannot auto-detect, using default 4 workers: {e}")
            return 4
    
    def _get_thread_local_service(self):
        """Create separate service instance for each thread"""
        return build('drive', 'v3', credentials=self.creds)

    def load_log(self):
        """Load backup log from JSON file"""
        if os.path.exists(self.log_file):
            with open(self.log_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {'backed_up_files': {}, 'last_run': None}

    def save_log(self):
        """Save backup log to JSON file (thread-safe)"""
        with self.log_lock:
            self.backup_log['last_run'] = datetime.now().isoformat()
            with open(self.log_file, 'w', encoding='utf-8') as f:
                json.dump(self.backup_log, f, indent=2, ensure_ascii=False)

    def calculate_md5(self, file_path):
        """Calculate MD5 checksum of file"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def get_file_info(self, file_id):
        """Get file information from Google Drive"""
        try:
            file = self.service.files().get(
                fileId=file_id,
                fields='id, name, size, md5Checksum, mimeType'
            ).execute()
            return file
        except HttpError as e:
            print(f"‚ùå Error getting file info {file_id}: {e}")
            return None

    def download_file(self, file_id, file_name, file_size=None, max_retries=3, service=None):
        """Download file from Google Drive with retry mechanism - Memory safe & Fast"""
        if service is None:
            service = self.service
            
        local_path = os.path.join(self.local_temp_dir, file_name)

        for attempt in range(max_retries):
            fh = None
            pbar = None
            
            try:
                request = service.files().get_media(fileId=file_id)
                fh = io.FileIO(local_path, 'wb')
                downloader = MediaIoBaseDownload(fh, request, chunksize=10*1024*1024)  # 10MB chunks for speed

                done = False
                pbar = tqdm(total=100, desc=f"üì• Downloading {file_name[:30]}...", unit='%', leave=False)

                while not done:
                    status, done = downloader.next_chunk()
                    if status:
                        pbar.update(int(status.progress() * 100) - pbar.n)

                pbar.close()
                pbar = None
                fh.close()
                fh = None

                # Validate file size if provided
                if file_size:
                    local_size = os.path.getsize(local_path)
                    if local_size != int(file_size):
                        raise Exception(f"File size mismatch: expected {file_size}, got {local_size}")

                print(f"‚úÖ Downloaded: {file_name}")
                return local_path

            except Exception as e:
                print(f"‚ö†Ô∏è  Download attempt {attempt + 1}/{max_retries} failed for {file_name}: {e}")
                
                # Cleanup partial download
                if os.path.exists(local_path):
                    try:
                        os.remove(local_path)
                    except:
                        pass
                
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    print(f"‚ùå Failed to download {file_name} after {max_retries} attempts")
                    return None
                    
            finally:
                # Fast cleanup - only if not already closed
                if pbar is not None:
                    try:
                        pbar.close()
                    except:
                        pass
                if fh is not None:
                    try:
                        fh.close()
                    except:
                        pass

    def upload_file(self, local_path, file_name, parent_folder_id, original_md5=None, max_retries=3, service=None):
        """Upload file to Google Drive with validation - Memory safe & Fast"""
        if service is None:
            service = self.service
            
        for attempt in range(max_retries):
            uploaded_file_id = None
            
            try:
                file_metadata = {
                    'name': file_name,
                    'parents': [parent_folder_id]
                }

                # Use larger chunksize for speed (5MB)
                media = MediaFileUpload(local_path, resumable=True, chunksize=5*1024*1024)
                file = service.files().create(
                    body=file_metadata,
                    media_body=media,
                    fields='id, name, size, md5Checksum'
                ).execute()
                
                uploaded_file_id = file['id']

                # Validate MD5 checksum if provided
                if original_md5 and file.get('md5Checksum') != original_md5:
                    try:
                        service.files().delete(fileId=uploaded_file_id).execute()
                    except:
                        pass
                    raise Exception("MD5 checksum mismatch")

                print(f"‚úÖ Uploaded: {file_name} (ID: {uploaded_file_id})")
                return uploaded_file_id

            except Exception as e:
                print(f"‚ö†Ô∏è  Upload attempt {attempt + 1}/{max_retries} failed for {file_name}: {e}")
                
                # Try to delete file if upload failed
                if uploaded_file_id:
                    try:
                        service.files().delete(fileId=uploaded_file_id).execute()
                    except:
                        pass
                
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    print(f"‚ùå Failed to upload {file_name} after {max_retries} attempts")
                    return None

    def create_folder(self, folder_name, parent_id=None):
        """Create new folder in Google Drive"""
        try:
            file_metadata = {
                'name': folder_name,
                'mimeType': 'application/vnd.google-apps.folder'
            }
            if parent_id:
                file_metadata['parents'] = [parent_id]

            folder = self.service.files().create(
                body=file_metadata,
                fields='id, name'
            ).execute()

            print(f"üìÅ Created folder: {folder_name} (ID: {folder['id']})")
            return folder['id']
        except HttpError as e:
            print(f"‚ùå Error creating folder: {e}")
            return None

    def list_files_in_folder(self, folder_id):
        """List all files and folders within a folder"""
        items = []
        page_token = None

        try:
            while True:
                response = self.service.files().list(
                    q=f"'{folder_id}' in parents and trashed=false",
                    fields='nextPageToken, files(id, name, mimeType, size, md5Checksum)',
                    pageToken=page_token,
                    pageSize=100
                ).execute()

                items.extend(response.get('files', []))
                page_token = response.get('nextPageToken')

                if not page_token:
                    break

            return items
        except HttpError as e:
            print(f"‚ùå Error listing files: {e}")
            return []

    def process_single_file(self, item, backup_folder_id):
        """Process a single file (download + upload + cleanup) - Fast & Safe"""
        item_id = item['id']
        item_name = item['name']
        file_size = item.get('size')
        original_md5 = item.get('md5Checksum')

        thread_service = None
        local_path = None
        
        try:
            thread_service = self._get_thread_local_service()
            
            # Check if already backed up
            with self.log_lock:
                if item_id in self.backup_log['backed_up_files']:
                    print(f"‚è≠Ô∏è  Skipped (already backed up): {item_name}")
                    self.download_stats['skipped'] += 1
                    return True

            # Download file
            local_path = self.download_file(item_id, item_name, file_size, service=thread_service)

            if local_path and os.path.exists(local_path):
                self.download_stats['success'] += 1
                
                # Upload file
                uploaded_id = self.upload_file(
                    local_path,
                    item_name,
                    backup_folder_id,
                    original_md5,
                    service=thread_service
                )

                if uploaded_id:
                    self.upload_stats['success'] += 1
                    
                    # Log successful backup
                    with self.log_lock:
                        self.backup_log['backed_up_files'][item_id] = {
                            'name': item_name,
                            'type': 'file',
                            'size': file_size,
                            'md5': original_md5,
                            'backup_id': uploaded_id,
                            'backup_time': datetime.now().isoformat()
                        }

                    # Delete local file
                    try:
                        os.remove(local_path)
                        local_path = None
                        print(f"üóëÔ∏è  Cleaned up local file: {item_name}")
                    except Exception as e:
                        print(f"‚ö†Ô∏è  Warning: Could not delete local file: {e}")

                    # Save log
                    self.save_log()
                    return True
                else:
                    self.upload_stats['failed'] += 1
                    print(f"‚ùå Failed to backup file: {item_name}")
                    with self.log_lock:
                        if hasattr(self, 'failed_files') and item not in self.failed_files:
                            self.failed_files.append(item)
                    return False
            else:
                self.download_stats['failed'] += 1
                print(f"‚ùå Failed to download file: {item_name}")
                with self.log_lock:
                    if hasattr(self, 'failed_files') and item not in self.failed_files:
                        self.failed_files.append(item)
                return False

        except Exception as e:
            print(f"‚ùå Error processing file {item_name}: {e}")
            with self.log_lock:
                if hasattr(self, 'failed_files') and item not in self.failed_files:
                    self.failed_files.append(item)
            return False
            
        finally:
            # Cleanup local file if still exists
            if local_path and os.path.exists(local_path):
                try:
                    os.remove(local_path)
                except:
                    pass

    def backup_folder(self, source_folder_id, backup_parent_id=None, folder_name_suffix='_BACKUP'):
        """Backup entire folder (recursive) with full verification"""
        # Get source folder information
        source_info = self.get_file_info(source_folder_id)
        if not source_info:
            print("‚ùå Cannot get source folder information")
            return None

        backup_folder_name = source_info['name'] + folder_name_suffix

        # Create backup folder
        backup_folder_id = self.create_folder(backup_folder_name, backup_parent_id)
        if not backup_folder_id:
            return None

        # Reset stats
        self.download_stats = {'success': 0, 'failed': 0, 'skipped': 0}
        self.upload_stats = {'success': 0, 'failed': 0}
        self.failed_files = []  # Track files that failed

        # Backup this folder
        self._backup_folder_recursive(source_folder_id, backup_folder_id)

        # Save log
        self.save_log()

        print(f"\nüéâ Completed backup of folder: {backup_folder_name}")
        print(f"\nüìä Download Stats: ‚úÖ {self.download_stats['success']} success | "
              f"‚ùå {self.download_stats['failed']} failed | "
              f"‚è≠Ô∏è  {self.download_stats['skipped']} skipped")
        print(f"üìä Upload Stats: ‚úÖ {self.upload_stats['success']} success | "
              f"‚ùå {self.upload_stats['failed']} failed")
        
        # Verification: Retry failed files
        if self.failed_files:
            print(f"\n‚ö†Ô∏è  {len(self.failed_files)} files failed. Retrying...")
            self._retry_failed_files(backup_folder_id)
        
        # Final verification
        self._verify_backup_completeness(source_folder_id, backup_folder_id)
        
        return backup_folder_id
    
    def _retry_failed_files(self, backup_folder_id, max_retry_attempts=2):
        """Retry failed files"""
        for attempt in range(max_retry_attempts):
            if not self.failed_files:
                break
                
            print(f"\nüîÑ Retry attempt {attempt + 1}/{max_retry_attempts} for {len(self.failed_files)} failed files")
            
            current_failed = self.failed_files.copy()
            self.failed_files = []
            
            for item in current_failed:
                result = self.process_single_file(item, backup_folder_id)
                if not result:
                    self.failed_files.append(item)
            
            if self.failed_files:
                print(f"‚ö†Ô∏è  Still {len(self.failed_files)} files failed after retry {attempt + 1}")
                time.sleep(2)  # Wait before next retry
        
        if self.failed_files:
            print(f"\n‚ùå WARNING: {len(self.failed_files)} files CANNOT be backed up after all retries:")
            for item in self.failed_files:
                print(f"   ‚ùå {item['name']} (ID: {item['id']})")
    
    def _verify_backup_completeness(self, source_folder_id, backup_folder_id):
        """Verify backup completeness by counting files"""
        print("\nüîç Verifying backup completeness...")
        
        source_count = self._count_all_files_recursive(source_folder_id)
        backup_count = self._count_all_files_recursive(backup_folder_id)
        
        print(f"üìÅ Source folder: {source_count['files']} files, {source_count['folders']} folders")
        print(f"üìÅ Backup folder: {backup_count['files']} files, {backup_count['folders']} folders")
        
        if source_count['files'] == backup_count['files']:
            print("‚úÖ VERIFICATION PASSED: All files have been backed up!")
        else:
            missing = source_count['files'] - backup_count['files']
            print(f"‚ö†Ô∏è  VERIFICATION WARNING: Missing {missing} files in backup!")
            print(f"‚ö†Ô∏è  Please check again or run backup once more.")
    
    def _count_all_files_recursive(self, folder_id):
        """Count all files and folders (recursive)"""
        items = self.list_files_in_folder(folder_id)
        
        count = {'files': 0, 'folders': 0}
        
        for item in items:
            if item['mimeType'] == 'application/vnd.google-apps.folder':
                count['folders'] += 1
                # Recurse into subfolder
                sub_count = self._count_all_files_recursive(item['id'])
                count['files'] += sub_count['files']
                count['folders'] += sub_count['folders']
            else:
                count['files'] += 1
        
        return count

    def _backup_folder_recursive(self, source_folder_id, backup_folder_id):
        """Backup folder recursively with multi-threaded file processing - Fast & Safe"""
        items = self.list_files_in_folder(source_folder_id)

        print(f"\nüìä Found {len(items)} items in folder")

        # Categorize files and folders
        folders = [item for item in items if item['mimeType'] == 'application/vnd.google-apps.folder']
        files = [item for item in items if item['mimeType'] != 'application/vnd.google-apps.folder']

        # Process folders first (not in parallel as structure needs to be created)
        for folder_item in folders:
            item_id = folder_item['id']
            item_name = folder_item['name']

            # Check if already backed up
            if item_id in self.backup_log['backed_up_files']:
                print(f"‚è≠Ô∏è  Skipped (already backed up): {item_name}")
                continue

            print(f"\nüìÅ Processing subfolder: {item_name}")
            new_folder_id = self.create_folder(item_name, backup_folder_id)
            
            if new_folder_id:
                self._backup_folder_recursive(item_id, new_folder_id)
                
                with self.log_lock:
                    self.backup_log['backed_up_files'][item_id] = {
                        'name': item_name,
                        'type': 'folder',
                        'backup_time': datetime.now().isoformat()
                    }

        # Process files in parallel with ThreadPoolExecutor
        if files:
            print(f"\nüöÄ Starting download of {len(files)} files with {self.max_workers} concurrent threads...")
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # Submit all file tasks
                future_to_file = {
                    executor.submit(self.process_single_file, file_item, backup_folder_id): file_item
                    for file_item in files
                }

                # Wait for all tasks to complete with timeout
                completed = 0
                for future in concurrent.futures.as_completed(future_to_file, timeout=3600):
                    file_item = future_to_file[future]
                    completed += 1
                    try:
                        result = future.result()
                        if not result:
                            print(f"‚ö†Ô∏è  Processing failed for: {file_item['name']}")
                    except concurrent.futures.TimeoutError:
                        print(f"‚è±Ô∏è  TIMEOUT processing {file_item['name']}")
                        with self.log_lock:
                            if hasattr(self, 'failed_files') and file_item not in self.failed_files:
                                self.failed_files.append(file_item)
                    except Exception as e:
                        print(f"‚ùå Exception processing {file_item['name']}: {e}")
                        with self.log_lock:
                            if hasattr(self, 'failed_files') and file_item not in self.failed_files:
                                self.failed_files.append(file_item)
                    
                    # Smart GC: only every 20 files and if memory > 80%
                    if completed % 20 == 0:
                        mem = psutil.virtual_memory().percent
                        if mem > 80:
                            gc.collect()
                
                print(f"\n‚úÖ Processed {completed}/{len(files)} files")
            
            # GC after large batches (>50 files)
            if len(files) > 50:
                gc.collect()

        # Short break between folders
        time.sleep(0.2)

    def get_backup_stats(self):
        """Display backup statistics"""
        total_files = len(self.backup_log['backed_up_files'])
        files_count = sum(1 for item in self.backup_log['backed_up_files'].values() if item['type'] == 'file')
        folders_count = sum(1 for item in self.backup_log['backed_up_files'].values() if item['type'] == 'folder')

        print("\n" + "="*50)
        print("üìä BACKUP STATISTICS")
        print("="*50)
        print(f"Total items backed up: {total_files}")
        print(f"  - Files: {files_count}")
        print(f"  - Folders: {folders_count}")
        print(f"Last run: {self.backup_log.get('last_run', 'Never')}")
        print("="*50 + "\n")
    
    def generate_backup_report(self):
        """Generate detailed backup report"""
        print("\n" + "="*60)
        print("üìã DETAILED BACKUP REPORT")
        print("="*60)
        
        total_items = len(self.backup_log['backed_up_files'])
        if total_items == 0:
            print("No items have been backed up yet.")
            return
        
        # Categorize by type
        files = [item for item in self.backup_log['backed_up_files'].values() if item['type'] == 'file']
        folders = [item for item in self.backup_log['backed_up_files'].values() if item['type'] == 'folder']
        
        print(f"\nüìÅ Total folders: {len(folders)}")
        print(f"üìÑ Total files: {len(files)}")
        
        # Calculate total size
        total_size = sum(int(f.get('size', 0)) for f in files if f.get('size'))
        size_gb = total_size / (1024**3)
        print(f"üíæ Total size: {size_gb:.2f} GB ({total_size:,} bytes)")
        
        # Check MD5
        files_with_md5 = sum(1 for f in files if f.get('md5'))
        print(f"‚úÖ Files with MD5 validation: {files_with_md5}/{len(files)}")
        
        print(f"\nüïê Last backup time: {self.backup_log.get('last_run', 'N/A')}")
        print("="*60 + "\n")

"""## Using the Program

### Configuration has been set at the top of the file
See the "‚öôÔ∏è MAIN CONFIGURATION" section at the beginning to modify:
- SOURCE_FOLDER_ID: ID of folder to backup
- BACKUP_PARENT_ID: Destination folder for backup
- FOLDER_SUFFIX: Folder name suffix
- MAX_WORKERS: Number of threads (None = auto)
"""

# Initialize Backup Manager (using settings from top of file)
backup_manager = DriveBackupManager(
    drive_service, 
    log_file='backup_log.json',
    max_workers=MAX_WORKERS  # None = auto-detect, or 4, 6, 8...
)

# Display previous backup statistics (if any)
backup_manager.get_backup_stats()

# START BACKUP
print("\nüöÄ Starting backup process...")
print(f"üìÅ Source: {SOURCE_FOLDER_ID}")
print(f"üìÅ Destination: {BACKUP_PARENT_ID or 'My Drive (root)'}")
print(f"üè∑Ô∏è  Suffix: {FOLDER_SUFFIX}\n")

start_time = time.time()

backup_folder_id = backup_manager.backup_folder(
    source_folder_id=SOURCE_FOLDER_ID,
    backup_parent_id=BACKUP_PARENT_ID,
    folder_name_suffix=FOLDER_SUFFIX
)

end_time = time.time()
duration = end_time - start_time

if backup_folder_id:
    print(f"\n‚úÖ BACKUP COMPLETED SUCCESSFULLY!")
    print(f"‚è±Ô∏è  Time taken: {duration:.2f} seconds ({duration/60:.2f} minutes)")
    print(f"üìÅ Backup folder ID: {backup_folder_id}")
    print(f"üîó Link: https://drive.google.com/drive/folders/{backup_folder_id}")
    
    # Generate detailed report
    backup_manager.generate_backup_report()
else:
    print("\n‚ùå BACKUP FAILED!")

# Display final backup statistics
backup_manager.get_backup_stats()

"""## Additional Utilities"""

# View backup log contents
if os.path.exists('backup_log.json'):
    with open('backup_log.json', 'r', encoding='utf-8') as f:
        log_data = json.load(f)
        print(json.dumps(log_data, indent=2, ensure_ascii=False))
else:
    print("‚ö†Ô∏è  Backup log does not exist yet. Please run backup at least once first.")

# Reset backup log (start from scratch)
# WARNING: Only run if you want to backup everything from the beginning!

# reset_log = {'backed_up_files': {}, 'last_run': None}
# with open('backup_log.json', 'w', encoding='utf-8') as f:
#     json.dump(reset_log, f, indent=2, ensure_ascii=False)
# print("üîÑ Backup log has been reset!")

# Download backup log to local machine
if os.path.exists('backup_log.json'):
    from google.colab import files
    files.download('backup_log.json')
    print("‚úÖ Downloaded backup log to your computer!")
else:
    print("‚ö†Ô∏è  Backup log does not exist yet. Nothing to download.")
