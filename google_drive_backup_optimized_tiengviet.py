# -*- coding: utf-8 -*-
"""google_drive_backup_optimized.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/xxxxxyyyyyzzzzzz

# Google Drive Folder Backup Tool - OPTIMIZED VERSION
## CÃ´ng cá»¥ sao lÆ°u folder tá»« Google Drive vá»›i validation, logging vÃ  táº£i xuá»‘ng Ä‘a luá»“ng

### TÃ­nh nÄƒng:
- âœ… Backup toÃ n bá»™ folder Ä‘Æ°á»£c share tá»« Google Drive
- âœ… Táº¡o folder backup vá»›i suffix "_BACKUP"
- âœ… Validation file trÆ°á»›c khi xÃ³a (kiá»ƒm tra size vÃ  checksum)
- âœ… JSON logging Ä‘á»ƒ trÃ¡nh backup trÃ¹ng láº·p
- âœ… Tá»± Ä‘á»™ng cleanup file local sau khi upload thÃ nh cÃ´ng
- âœ… Retry mechanism cho cÃ¡c lá»—i network
- âœ… Progress tracking chi tiáº¿t
- ğŸš€ **Má»šI: Táº£i xuá»‘ng Ä‘a luá»“ng (3-5 files cÃ¹ng lÃºc)**
- ğŸš€ **Má»šI: Tá»± Ä‘á»™ng tá»‘i Æ°u sá»‘ workers dá»±a trÃªn RAM/CPU**
- ğŸš€ **Má»šI: KhÃ´ng cÃ³ timeout warnings**
"""

# ============================================================
# âš™ï¸  Cáº¤U HÃŒNH CHÃNH - CHá»ˆNH Sá»¬A á» ÄÃ‚Y
# ============================================================

# ğŸ“ ID cá»§a folder cáº§n backup (láº¥y tá»« URL Google Drive)
# VÃ­ dá»¥: https://drive.google.com/drive/folders/1ZY4ab0XlPHa5_t10XnSvPbWUvJRdN4Nx
SOURCE_FOLDER_ID = 'xxxxxxzzzzzzzzzzyyyyyyyyySOURCEfolderID'

# ğŸ“ ID cá»§a folder cha nÆ¡i lÆ°u backup (tÃ¹y chá»n)
# Äá»ƒ None náº¿u muá»‘n lÆ°u á»Ÿ thÆ° má»¥c gá»‘c "My Drive"
BACKUP_PARENT_ID = 'xxxxxxzzzzzzzzzzyyyyyyyyyBACKUPfolderID'

# ğŸ·ï¸  Suffix cho tÃªn folder backup
FOLDER_SUFFIX = '_BACKUP'

# ğŸš€ Sá»‘ luá»“ng download Ä‘á»“ng thá»i (None = tá»± Ä‘á»™ng tá»‘i Æ°u)
# GiÃ¡ trá»‹ Ä‘á» xuáº¥t: 3-8 (hoáº·c Ä‘á»ƒ None Ä‘á»ƒ tá»± Ä‘á»™ng)
MAX_WORKERS = None  # None = auto-detect, hoáº·c set 4, 6, 8...

# ============================================================

# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
!pip install -q google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client tqdm requests psutil

# Import cÃ¡c thÆ° viá»‡n
import os
import json
import hashlib
import time
from datetime import datetime
from pathlib import Path
from tqdm.notebook import tqdm
from google.colab import auth
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
from googleapiclient.errors import HttpError
from google.auth import default
from google.auth.transport.requests import AuthorizedSession
import requests
import io
import concurrent.futures
from threading import Lock
import logging
import psutil
import multiprocessing
import gc  # Garbage collector for memory management

# Táº¯t warning vá» timeout tá»« google_auth_httplib2
logging.getLogger('google_auth_httplib2').setLevel(logging.ERROR)

# XÃ¡c thá»±c vá»›i Google Drive
from google.auth import default
from google.auth.transport.requests import Request

auth.authenticate_user()

# Láº¥y credentials
creds, _ = default()

# Build service vá»›i credentials (khÃ´ng cáº§n HTTP client riÃªng)
drive_service = build('drive', 'v3', credentials=creds)
print("âœ… ÄÃ£ xÃ¡c thá»±c thÃ nh cÃ´ng vá»›i Google Drive!")

class DriveBackupManager:
    """Quáº£n lÃ½ viá»‡c backup folder tá»« Google Drive vá»›i táº£i xuá»‘ng Ä‘a luá»“ng"""

    def __init__(self, service, log_file='backup_log.json', max_workers=None):
        self.service = service
        self.log_file = log_file
        self.backup_log = self.load_log()
        self.local_temp_dir = '/content/temp_backup'
        os.makedirs(self.local_temp_dir, exist_ok=True)
        
        # Tá»± Ä‘á»™ng phÃ¡t hiá»‡n sá»‘ workers tá»‘i Æ°u
        if max_workers is None:
            self.max_workers = self._auto_detect_workers()
        else:
            self.max_workers = max_workers
            
        self.log_lock = Lock()  # Lock Ä‘á»ƒ Ä‘á»“ng bá»™ ghi log
        self.download_stats = {'success': 0, 'failed': 0, 'skipped': 0}
        self.upload_stats = {'success': 0, 'failed': 0}
        # Store credentials for creating thread-local services
        self.creds, _ = default()
        
        print(f"ğŸš€ Sá»‘ workers Ä‘Æ°á»£c sá»­ dá»¥ng: {self.max_workers}")
    
    def __del__(self):
        """Destructor - cleanup resources when object is destroyed"""
        try:
            # Cleanup temp directory
            if hasattr(self, 'local_temp_dir') and os.path.exists(self.local_temp_dir):
                for file in os.listdir(self.local_temp_dir):
                    file_path = os.path.join(self.local_temp_dir, file)
                    try:
                        os.remove(file_path)
                    except:
                        pass
            
            # Force garbage collection
            gc.collect()
        except:
            pass
    
    def _auto_detect_workers(self):
        """Tá»± Ä‘á»™ng phÃ¡t hiá»‡n sá»‘ workers tá»‘i Æ°u dá»±a trÃªn RAM vÃ  CPU"""
        try:
            # Láº¥y thÃ´ng tin RAM
            memory = psutil.virtual_memory()
            available_gb = memory.available / (1024 ** 3)
            
            # Láº¥y sá»‘ CPU cores
            cpu_count = multiprocessing.cpu_count()
            
            # TÃ­nh toÃ¡n sá»‘ workers tá»‘i Æ°u
            # Má»—i worker cáº§n khoáº£ng 200-300MB RAM cho buffer
            workers_by_ram = int(available_gb / 0.3)  # 300MB per worker
            
            # Sá»‘ workers khÃ´ng vÆ°á»£t quÃ¡ sá»‘ CPU cores
            workers_by_cpu = cpu_count
            
            # Chá»n min nhÆ°ng Ä‘áº£m báº£o trong khoáº£ng an toÃ n 3-8
            optimal_workers = min(workers_by_ram, workers_by_cpu)
            optimal_workers = max(3, min(optimal_workers, 8))  # Giá»›i háº¡n 3-8
            
            print(f"ğŸ’¾ RAM kháº£ dá»¥ng: {available_gb:.1f} GB")
            print(f"ğŸ–¥ï¸  CPU cores: {cpu_count}")
            print(f"âš™ï¸  Workers tá»‘i Æ°u: {optimal_workers} (dá»±a trÃªn RAM: {workers_by_ram}, CPU: {workers_by_cpu})")
            
            return optimal_workers
            
        except Exception as e:
            print(f"âš ï¸  KhÃ´ng thá»ƒ tá»± Ä‘á»™ng phÃ¡t hiá»‡n, sá»­ dá»¥ng máº·c Ä‘á»‹nh 4 workers: {e}")
            return 4
    
    def _get_thread_local_service(self):
        """Táº¡o service instance riÃªng cho má»—i thread"""
        return build('drive', 'v3', credentials=self.creds)

    def load_log(self):
        """Load backup log tá»« file JSON"""
        if os.path.exists(self.log_file):
            with open(self.log_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {'backed_up_files': {}, 'last_run': None}

    def save_log(self):
        """LÆ°u backup log vÃ o file JSON (thread-safe)"""
        with self.log_lock:
            self.backup_log['last_run'] = datetime.now().isoformat()
            with open(self.log_file, 'w', encoding='utf-8') as f:
                json.dump(self.backup_log, f, indent=2, ensure_ascii=False)

    def calculate_md5(self, file_path):
        """TÃ­nh MD5 checksum cá»§a file"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def get_file_info(self, file_id):
        """Láº¥y thÃ´ng tin file tá»« Google Drive"""
        try:
            file = self.service.files().get(
                fileId=file_id,
                fields='id, name, size, md5Checksum, mimeType'
            ).execute()
            return file
        except HttpError as e:
            print(f"âŒ Lá»—i khi láº¥y thÃ´ng tin file {file_id}: {e}")
            return None

    def download_file(self, file_id, file_name, file_size=None, max_retries=3, service=None):
        """Download file tá»« Google Drive vá»›i retry mechanism - Memory safe & Fast"""
        if service is None:
            service = self.service
            
        local_path = os.path.join(self.local_temp_dir, file_name)

        for attempt in range(max_retries):
            fh = None
            pbar = None
            
            try:
                request = service.files().get_media(fileId=file_id)
                fh = io.FileIO(local_path, 'wb')
                downloader = MediaIoBaseDownload(fh, request, chunksize=10*1024*1024)  # 10MB chunks for speed

                done = False
                pbar = tqdm(total=100, desc=f"ğŸ“¥ Downloading {file_name[:30]}...", unit='%', leave=False)

                while not done:
                    status, done = downloader.next_chunk()
                    if status:
                        pbar.update(int(status.progress() * 100) - pbar.n)

                pbar.close()
                pbar = None
                fh.close()
                fh = None

                # Validate file size náº¿u cÃ³
                if file_size:
                    local_size = os.path.getsize(local_path)
                    if local_size != int(file_size):
                        raise Exception(f"File size mismatch: expected {file_size}, got {local_size}")

                print(f"âœ… Downloaded: {file_name}")
                return local_path

            except Exception as e:
                print(f"âš ï¸  Download attempt {attempt + 1}/{max_retries} failed for {file_name}: {e}")
                
                # Cleanup partial download
                if os.path.exists(local_path):
                    try:
                        os.remove(local_path)
                    except:
                        pass
                
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    print(f"âŒ Failed to download {file_name} after {max_retries} attempts")
                    return None
                    
            finally:
                # Fast cleanup - only if not already closed
                if pbar is not None:
                    try:
                        pbar.close()
                    except:
                        pass
                if fh is not None:
                    try:
                        fh.close()
                    except:
                        pass

    def upload_file(self, local_path, file_name, parent_folder_id, original_md5=None, max_retries=3, service=None):
        """Upload file lÃªn Google Drive vá»›i validation - Memory safe & Fast"""
        if service is None:
            service = self.service
            
        for attempt in range(max_retries):
            uploaded_file_id = None
            
            try:
                file_metadata = {
                    'name': file_name,
                    'parents': [parent_folder_id]
                }

                # Use larger chunksize for speed (5MB)
                media = MediaFileUpload(local_path, resumable=True, chunksize=5*1024*1024)
                file = service.files().create(
                    body=file_metadata,
                    media_body=media,
                    fields='id, name, size, md5Checksum'
                ).execute()
                
                uploaded_file_id = file['id']

                # Validate MD5 checksum náº¿u cÃ³
                if original_md5 and file.get('md5Checksum') != original_md5:
                    try:
                        service.files().delete(fileId=uploaded_file_id).execute()
                    except:
                        pass
                    raise Exception("MD5 checksum mismatch")

                print(f"âœ… Uploaded: {file_name} (ID: {uploaded_file_id})")
                return uploaded_file_id

            except Exception as e:
                print(f"âš ï¸  Upload attempt {attempt + 1}/{max_retries} failed for {file_name}: {e}")
                
                # Thá»­ xÃ³a file náº¿u upload lá»—i
                if uploaded_file_id:
                    try:
                        service.files().delete(fileId=uploaded_file_id).execute()
                    except:
                        pass
                
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                else:
                    print(f"âŒ Failed to upload {file_name} after {max_retries} attempts")
                    return None

    def create_folder(self, folder_name, parent_id=None):
        """Táº¡o folder má»›i trong Google Drive"""
        try:
            file_metadata = {
                'name': folder_name,
                'mimeType': 'application/vnd.google-apps.folder'
            }
            if parent_id:
                file_metadata['parents'] = [parent_id]

            folder = self.service.files().create(
                body=file_metadata,
                fields='id, name'
            ).execute()

            print(f"ğŸ“ Created folder: {folder_name} (ID: {folder['id']})")
            return folder['id']
        except HttpError as e:
            print(f"âŒ Error creating folder: {e}")
            return None

    def list_files_in_folder(self, folder_id):
        """Liá»‡t kÃª táº¥t cáº£ files vÃ  folders trong má»™t folder"""
        items = []
        page_token = None

        try:
            while True:
                response = self.service.files().list(
                    q=f"'{folder_id}' in parents and trashed=false",
                    fields='nextPageToken, files(id, name, mimeType, size, md5Checksum)',
                    pageToken=page_token,
                    pageSize=100
                ).execute()

                items.extend(response.get('files', []))
                page_token = response.get('nextPageToken')

                if not page_token:
                    break

            return items
        except HttpError as e:
            print(f"âŒ Error listing files: {e}")
            return []

    def process_single_file(self, item, backup_folder_id):
        """Xá»­ lÃ½ má»™t file Ä‘Æ¡n láº» (download + upload + cleanup) - Fast & Safe"""
        item_id = item['id']
        item_name = item['name']
        file_size = item.get('size')
        original_md5 = item.get('md5Checksum')

        thread_service = None
        local_path = None
        
        try:
            thread_service = self._get_thread_local_service()
            
            # Kiá»ƒm tra Ä‘Ã£ backup chÆ°a
            with self.log_lock:
                if item_id in self.backup_log['backed_up_files']:
                    print(f"â­ï¸  Skipped (already backed up): {item_name}")
                    self.download_stats['skipped'] += 1
                    return True

            # Download file
            local_path = self.download_file(item_id, item_name, file_size, service=thread_service)

            if local_path and os.path.exists(local_path):
                self.download_stats['success'] += 1
                
                # Upload file
                uploaded_id = self.upload_file(
                    local_path,
                    item_name,
                    backup_folder_id,
                    original_md5,
                    service=thread_service
                )

                if uploaded_id:
                    self.upload_stats['success'] += 1
                    
                    # Log backup thÃ nh cÃ´ng
                    with self.log_lock:
                        self.backup_log['backed_up_files'][item_id] = {
                            'name': item_name,
                            'type': 'file',
                            'size': file_size,
                            'md5': original_md5,
                            'backup_id': uploaded_id,
                            'backup_time': datetime.now().isoformat()
                        }

                    # XÃ³a file local
                    try:
                        os.remove(local_path)
                        local_path = None
                        print(f"ğŸ—‘ï¸  Cleaned up local file: {item_name}")
                    except Exception as e:
                        print(f"âš ï¸  Warning: Could not delete local file: {e}")

                    # LÆ°u log
                    self.save_log()
                    return True
                else:
                    self.upload_stats['failed'] += 1
                    print(f"âŒ Failed to backup file: {item_name}")
                    with self.log_lock:
                        if hasattr(self, 'failed_files') and item not in self.failed_files:
                            self.failed_files.append(item)
                    return False
            else:
                self.download_stats['failed'] += 1
                print(f"âŒ Failed to download file: {item_name}")
                with self.log_lock:
                    if hasattr(self, 'failed_files') and item not in self.failed_files:
                        self.failed_files.append(item)
                return False

        except Exception as e:
            print(f"âŒ Error processing file {item_name}: {e}")
            with self.log_lock:
                if hasattr(self, 'failed_files') and item not in self.failed_files:
                    self.failed_files.append(item)
            return False
            
        finally:
            # Cleanup local file if still exists
            if local_path and os.path.exists(local_path):
                try:
                    os.remove(local_path)
                except:
                    pass

    def backup_folder(self, source_folder_id, backup_parent_id=None, folder_name_suffix='_BACKUP'):
        """Backup toÃ n bá»™ folder (Ä‘á»‡ quy) vá»›i verification Ä‘áº§y Ä‘á»§"""
        # Láº¥y thÃ´ng tin folder gá»‘c
        source_info = self.get_file_info(source_folder_id)
        if not source_info:
            print("âŒ KhÃ´ng thá»ƒ láº¥y thÃ´ng tin folder gá»‘c")
            return None

        backup_folder_name = source_info['name'] + folder_name_suffix

        # Táº¡o folder backup
        backup_folder_id = self.create_folder(backup_folder_name, backup_parent_id)
        if not backup_folder_id:
            return None

        # Reset stats
        self.download_stats = {'success': 0, 'failed': 0, 'skipped': 0}
        self.upload_stats = {'success': 0, 'failed': 0}
        self.failed_files = []  # Track files that failed

        # Backup folder nÃ y
        self._backup_folder_recursive(source_folder_id, backup_folder_id)

        # LÆ°u log
        self.save_log()

        print(f"\nğŸ‰ HoÃ n thÃ nh backup folder: {backup_folder_name}")
        print(f"\nğŸ“Š Download Stats: âœ… {self.download_stats['success']} success | "
              f"âŒ {self.download_stats['failed']} failed | "
              f"â­ï¸  {self.download_stats['skipped']} skipped")
        print(f"ğŸ“Š Upload Stats: âœ… {self.upload_stats['success']} success | "
              f"âŒ {self.upload_stats['failed']} failed")
        
        # Verification: Retry failed files
        if self.failed_files:
            print(f"\nâš ï¸  CÃ³ {len(self.failed_files)} file bá»‹ lá»—i. Äang thá»­ láº¡i...")
            self._retry_failed_files(backup_folder_id)
        
        # Final verification
        self._verify_backup_completeness(source_folder_id, backup_folder_id)
        
        return backup_folder_id
    
    def _retry_failed_files(self, backup_folder_id, max_retry_attempts=2):
        """Retry cÃ¡c files bá»‹ lá»—i"""
        for attempt in range(max_retry_attempts):
            if not self.failed_files:
                break
                
            print(f"\nğŸ”„ Retry attempt {attempt + 1}/{max_retry_attempts} for {len(self.failed_files)} failed files")
            
            current_failed = self.failed_files.copy()
            self.failed_files = []
            
            for item in current_failed:
                result = self.process_single_file(item, backup_folder_id)
                if not result:
                    self.failed_files.append(item)
            
            if self.failed_files:
                print(f"âš ï¸  Still {len(self.failed_files)} files failed after retry {attempt + 1}")
                time.sleep(2)  # Wait before next retry
        
        if self.failed_files:
            print(f"\nâŒ Cáº¢NH BÃO: {len(self.failed_files)} files KHÃ”NG THá»‚ backup sau táº¥t cáº£ retry:")
            for item in self.failed_files:
                print(f"   âŒ {item['name']} (ID: {item['id']})")
    
    def _verify_backup_completeness(self, source_folder_id, backup_folder_id):
        """Verify backup hoÃ n chá»‰nh báº±ng cÃ¡ch Ä‘áº¿m files"""
        print("\nğŸ” Äang verify tÃ­nh hoÃ n chá»‰nh cá»§a backup...")
        
        source_count = self._count_all_files_recursive(source_folder_id)
        backup_count = self._count_all_files_recursive(backup_folder_id)
        
        print(f"ğŸ“ Source folder: {source_count['files']} files, {source_count['folders']} folders")
        print(f"ğŸ“ Backup folder: {backup_count['files']} files, {backup_count['folders']} folders")
        
        if source_count['files'] == backup_count['files']:
            print("âœ… VERIFICATION PASSED: Táº¥t cáº£ files Ä‘Ã£ Ä‘Æ°á»£c backup!")
        else:
            missing = source_count['files'] - backup_count['files']
            print(f"âš ï¸  VERIFICATION WARNING: Thiáº¿u {missing} files trong backup!")
            print(f"âš ï¸  Vui lÃ²ng kiá»ƒm tra láº¡i hoáº·c cháº¡y backup láº§n ná»¯a.")
    
    def _count_all_files_recursive(self, folder_id):
        """Äáº¿m táº¥t cáº£ files vÃ  folders (Ä‘á»‡ quy)"""
        items = self.list_files_in_folder(folder_id)
        
        count = {'files': 0, 'folders': 0}
        
        for item in items:
            if item['mimeType'] == 'application/vnd.google-apps.folder':
                count['folders'] += 1
                # Äá»‡ quy vÃ o subfolder
                sub_count = self._count_all_files_recursive(item['id'])
                count['files'] += sub_count['files']
                count['folders'] += sub_count['folders']
            else:
                count['files'] += 1
        
        return count

    def _backup_folder_recursive(self, source_folder_id, backup_folder_id):
        """Backup folder Ä‘á»‡ quy vá»›i xá»­ lÃ½ Ä‘a luá»“ng cho files - Fast & Safe"""
        items = self.list_files_in_folder(source_folder_id)

        print(f"\nğŸ“Š TÃ¬m tháº¥y {len(items)} items trong folder")

        # PhÃ¢n loáº¡i files vÃ  folders
        folders = [item for item in items if item['mimeType'] == 'application/vnd.google-apps.folder']
        files = [item for item in items if item['mimeType'] != 'application/vnd.google-apps.folder']

        # Xá»­ lÃ½ folders trÆ°á»›c (khÃ´ng song song vÃ¬ cáº§n táº¡o cáº¥u trÃºc)
        for folder_item in folders:
            item_id = folder_item['id']
            item_name = folder_item['name']

            # Kiá»ƒm tra Ä‘Ã£ backup chÆ°a
            if item_id in self.backup_log['backed_up_files']:
                print(f"â­ï¸  Skipped (already backed up): {item_name}")
                continue

            print(f"\nğŸ“ Processing subfolder: {item_name}")
            new_folder_id = self.create_folder(item_name, backup_folder_id)
            
            if new_folder_id:
                self._backup_folder_recursive(item_id, new_folder_id)
                
                with self.log_lock:
                    self.backup_log['backed_up_files'][item_id] = {
                        'name': item_name,
                        'type': 'folder',
                        'backup_time': datetime.now().isoformat()
                    }

        # Xá»­ lÃ½ files song song vá»›i ThreadPoolExecutor
        if files:
            print(f"\nğŸš€ Báº¯t Ä‘áº§u táº£i xuá»‘ng {len(files)} files vá»›i {self.max_workers} luá»“ng Ä‘á»“ng thá»i...")
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # Submit táº¥t cáº£ cÃ¡c file tasks
                future_to_file = {
                    executor.submit(self.process_single_file, file_item, backup_folder_id): file_item
                    for file_item in files
                }

                # Äá»£i táº¥t cáº£ tasks hoÃ n thÃ nh vá»›i timeout
                completed = 0
                for future in concurrent.futures.as_completed(future_to_file, timeout=3600):
                    file_item = future_to_file[future]
                    completed += 1
                    try:
                        result = future.result()
                        if not result:
                            print(f"âš ï¸  Processing failed for: {file_item['name']}")
                    except concurrent.futures.TimeoutError:
                        print(f"â±ï¸  TIMEOUT processing {file_item['name']}")
                        with self.log_lock:
                            if hasattr(self, 'failed_files') and file_item not in self.failed_files:
                                self.failed_files.append(file_item)
                    except Exception as e:
                        print(f"âŒ Exception processing {file_item['name']}: {e}")
                        with self.log_lock:
                            if hasattr(self, 'failed_files') and file_item not in self.failed_files:
                                self.failed_files.append(file_item)
                    
                    # Smart GC: only every 20 files and if memory > 80%
                    if completed % 20 == 0:
                        mem = psutil.virtual_memory().percent
                        if mem > 80:
                            gc.collect()
                
                print(f"\nâœ… ÄÃ£ xá»­ lÃ½ {completed}/{len(files)} files")
            
            # GC after large batches (>50 files)
            if len(files) > 50:
                gc.collect()

        # Nghá»‰ ngáº¯n giá»¯a cÃ¡c folder
        time.sleep(0.2)

    def get_backup_stats(self):
        """Hiá»ƒn thá»‹ thá»‘ng kÃª backup"""
        total_files = len(self.backup_log['backed_up_files'])
        files_count = sum(1 for item in self.backup_log['backed_up_files'].values() if item['type'] == 'file')
        folders_count = sum(1 for item in self.backup_log['backed_up_files'].values() if item['type'] == 'folder')

        print("\n" + "="*50)
        print("ğŸ“Š BACKUP STATISTICS")
        print("="*50)
        print(f"Total items backed up: {total_files}")
        print(f"  - Files: {files_count}")
        print(f"  - Folders: {folders_count}")
        print(f"Last run: {self.backup_log.get('last_run', 'Never')}")
        print("="*50 + "\n")
    
    def generate_backup_report(self):
        """Táº¡o bÃ¡o cÃ¡o chi tiáº¿t vá» backup"""
        print("\n" + "="*60)
        print("ğŸ“‹ CHI TIáº¾T BÃO CÃO BACKUP")
        print("="*60)
        
        total_items = len(self.backup_log['backed_up_files'])
        if total_items == 0:
            print("ChÆ°a cÃ³ item nÃ o Ä‘Æ°á»£c backup.")
            return
        
        # PhÃ¢n loáº¡i theo type
        files = [item for item in self.backup_log['backed_up_files'].values() if item['type'] == 'file']
        folders = [item for item in self.backup_log['backed_up_files'].values() if item['type'] == 'folder']
        
        print(f"\nğŸ“ Tá»•ng sá»‘ folders: {len(folders)}")
        print(f"ğŸ“„ Tá»•ng sá»‘ files: {len(files)}")
        
        # TÃ­nh tá»•ng dung lÆ°á»£ng
        total_size = sum(int(f.get('size', 0)) for f in files if f.get('size'))
        size_gb = total_size / (1024**3)
        print(f"ğŸ’¾ Tá»•ng dung lÆ°á»£ng: {size_gb:.2f} GB ({total_size:,} bytes)")
        
        # Kiá»ƒm tra MD5
        files_with_md5 = sum(1 for f in files if f.get('md5'))
        print(f"âœ… Files cÃ³ MD5 validation: {files_with_md5}/{len(files)}")
        
        print(f"\nğŸ• Thá»i gian backup gáº§n nháº¥t: {self.backup_log.get('last_run', 'N/A')}")
        print("="*60 + "\n")

"""## Sá»­ dá»¥ng chÆ°Æ¡ng trÃ¬nh

### Cáº¥u hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c Ä‘áº·t á»Ÿ Ä‘áº§u file
Xem pháº§n "âš™ï¸ Cáº¤U HÃŒNH CHÃNH" á»Ÿ Ä‘áº§u chÆ°Æ¡ng trÃ¬nh Ä‘á»ƒ thay Ä‘á»•i:
- SOURCE_FOLDER_ID: ID folder cáº§n backup
- BACKUP_PARENT_ID: Folder Ä‘Ã­ch lÆ°u backup
- FOLDER_SUFFIX: Háº­u tá»‘ tÃªn folder
- MAX_WORKERS: Sá»‘ luá»“ng (None = auto)
"""

# Khá»Ÿi táº¡o Backup Manager (sá»­ dá»¥ng settings tá»« Ä‘áº§u file)
backup_manager = DriveBackupManager(
    drive_service, 
    log_file='backup_log.json',
    max_workers=MAX_WORKERS  # None = auto-detect, hoáº·c 4, 6, 8...
)

# Hiá»ƒn thá»‹ thá»‘ng kÃª backup trÆ°á»›c Ä‘Ã³ (náº¿u cÃ³)
backup_manager.get_backup_stats()

# Báº®T Äáº¦U BACKUP
print("\nğŸš€ Starting backup process...")
print(f"ğŸ“ Source: {SOURCE_FOLDER_ID}")
print(f"ğŸ“ Destination: {BACKUP_PARENT_ID or 'My Drive (root)'}")
print(f"ğŸ·ï¸  Suffix: {FOLDER_SUFFIX}\n")

start_time = time.time()

backup_folder_id = backup_manager.backup_folder(
    source_folder_id=SOURCE_FOLDER_ID,
    backup_parent_id=BACKUP_PARENT_ID,
    folder_name_suffix=FOLDER_SUFFIX
)

end_time = time.time()
duration = end_time - start_time

if backup_folder_id:
    print(f"\nâœ… BACKUP COMPLETED SUCCESSFULLY!")
    print(f"â±ï¸  Time taken: {duration:.2f} seconds ({duration/60:.2f} minutes)")
    print(f"ğŸ“ Backup folder ID: {backup_folder_id}")
    print(f"ğŸ”— Link: https://drive.google.com/drive/folders/{backup_folder_id}")
    
    # Táº¡o bÃ¡o cÃ¡o chi tiáº¿t
    backup_manager.generate_backup_report()
else:
    print("\nâŒ BACKUP FAILED!")

# Hiá»ƒn thá»‹ thá»‘ng kÃª backup cuá»‘i cÃ¹ng
backup_manager.get_backup_stats()

"""## Tiá»‡n Ã­ch bá»• sung"""

# Xem ná»™i dung backup log
if os.path.exists('backup_log.json'):
    with open('backup_log.json', 'r', encoding='utf-8') as f:
        log_data = json.load(f)
        print(json.dumps(log_data, indent=2, ensure_ascii=False))
else:
    print("âš ï¸  Backup log chÆ°a tá»“n táº¡i. HÃ£y cháº¡y backup Ã­t nháº¥t má»™t láº§n trÆ°á»›c.")

# Reset backup log (cháº¡y láº¡i tá»« Ä‘áº§u)
# Cáº¢NH BÃO: Chá»‰ cháº¡y náº¿u báº¡n muá»‘n backup láº¡i toÃ n bá»™ tá»« Ä‘áº§u!

# reset_log = {'backed_up_files': {}, 'last_run': None}
# with open('backup_log.json', 'w', encoding='utf-8') as f:
#     json.dump(reset_log, f, indent=2, ensure_ascii=False)
# print("ğŸ”„ Backup log has been reset!")

# Download backup log vá» mÃ¡y
if os.path.exists('backup_log.json'):
    from google.colab import files
    files.download('backup_log.json')
    print("âœ… ÄÃ£ táº£i backup log vá» mÃ¡y!")
else:
    print("âš ï¸  Backup log chÆ°a tá»“n táº¡i. KhÃ´ng cÃ³ gÃ¬ Ä‘á»ƒ táº£i xuá»‘ng.")